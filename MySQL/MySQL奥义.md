# 第一节：MySQL奥义

### 基础

#### SQL执行流程

- 客户端
- 连接器（管理连接，权限验证）
- 查询缓存（命中则直接返回结果）
- 分析器（词法分析，语法分析）
- 优化器（执行计划生成，索引选择）
- 执行器（操作引擎，返回结果）
- 存储引擎（存储数据，提供读写接口）

#### char与varchar

* char和varchar 后面的长度表示的是字符的个数，而不是字节数。
* 如果某个数据表里的数据行的长度是可变的，那么，为了节约存储空间，MySQL会把这个数据表里的固定长度类型的数据列转换为相应的可变长度类型．例外：长度小于4个字符的char数据列不会被转换为varchar类型。
* 区别：
  * char 表示定长，长度固定，varchar表示变长，即长度可变
  * char最多能存放的**字符**个数 255，和编码无关；varchar最大有效长度是 65532 字节，在varchar存字符串的时候，第一个字节是空的，不存任何的数据，然后还需要两个字节（长度<255用一个字节存储长度，>255需要两个字节）来存放字符串的长度。所以有效长度就是 65535 - 1 - 2 = 65532字节，字符根据编码来定。
  * char的效率高，没有碎片，尤其更新比较频繁的时候，方便数据文件指针的操作，varchar更新数据需要重新计算长度
  * varchar相对来说比较灵活，可动态分配空间，char设置长度不合理时可能会浪费空间
* 如果已经对含有可变长度的表（varchar、blob、text）进行了很多更改，可以使用OPTIMIZE TABLE重新利用未使用的空间，并整理数据文件的碎片。

#### 编码

mysql8.0之前，默认的CHARSET是Latin1，默认的COLLATE是latin1_swedish_ci；从mysql8.0开始，默认的CHARSET已经改为了utf8mb4，默认的COLLATE改为了utf8mb4_0900_ai_ci

utf8_bin:将字符串中的每一个字符用二进制数据存储，区分大小写(在二进制中 ,小写字母 和大写字母 不相等.即 a !=A)。

utf8_genera_ci（常用）:不区分大小写，ci为case insensitive的缩写（insensitive ; 中文解释: adj. 感觉迟钝的，对…没有感觉的），即大小写不敏感。是一个遗留的 校对规则，不支持扩展。它仅能够在字符之间进行逐个比较。这意味着utf8_general_ci校对规则进行的比较速度很快，但是与使用utf8_unicode_ci的 校对规则相比，正确性较差）。

utf8_general_cs（一般不用）:区分大小写，cs为case sensitive的缩写（sensitive 中文解释:敏感事件;大小写敏感;注重大小写;全字拼写须符合），即大小写敏感；

utf8_unicode_ci：仅部分支持Unicode校对规则算法，一些字符还是不能支持。并且，不能完全支持组合的记号。这主要影响越南和俄罗斯的一些少数民族语言，如：Udmurt 、Tatar、Bashkir和Mari。utf8_unicode_ci的最主要的特色是支持扩展，即当把一个字母看作与其它字母组合相等时。例如，在德语和一些其它语言中‘ß'等于‘ss'。

#### redo/undo log与binlog

* redo log是innodb引擎提供的，binlog是mysql server自带的；

* redo log 记录着磁盘数据的变更日志（把表空间10、页号5、偏移量为10处的值更新为18）;binlog 记录 是怎么修改的（记录sql语句或者 记录更新前后的行）

* redo log是固定大小，从头开始写，写到末尾就又回到开头**循环写**（写满了要擦除一部分，擦除前数据要同步到磁盘）

* redo log ，主要解决解决异常、宕机而可能造成数据错误或丢失，异常重启，都会刷新到磁盘（满血复活）；而 bin log ， 则主要用于备份、验证、恢复、同步数据（主从同步）

* Undo Log 的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到一个地方（这个存储数据备份的地方称为 Undo Log）；数据里面会记录操作该数据的事务ID，然后我们可以通过事务ID来对数据进行回滚；

  undo log用途：

  - 事务回滚时，保证原子性和一致性。
  - 如果当前记录行不可见，可以顺着undo log链找到满足其可见性条件的记录行版本(用于MVCC快照读)。

#### binlog三种格式

SQL：delete from t where a>=4 and t_modified<='2018-11-10' **limit 1**;

- binlog_format=statement：statement 格式下，记录到 binlog 里的是SQL语句的原文，可能出现一种情况：在主库执行语句时，使用索引a；而在备库执行语句时，使用索引t_modified，SQL写成这样是有风险的（导致主从不一致）。
- binlog_format=‘row’：这时binlog 里面记录了真实删除行的主键 id，这样 binlog传到备库去的时候，肯定会删除指定ID对应的数据；row 格式的缺点是，很占空间，比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间，如果用row格式，就要把10万记录都写到binlog中，占用空间大，写binlog也耗费IO资源，影响执行速度。
- binlog_format=‘mixed’：MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。

#### update语句的内部流程

mysql> update T set c=c+1 where id=2;

1.执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回；

2.记录undo log 日志

3.同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。

4.引擎将这行新数据更新到内存中，

5.执行器生成这个操作的 binlog，并把 binlog 写入磁盘；

6.执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

##### 如果在上面的某一个阶段数据库崩溃，如何恢复数据

- 在第一步、第二步、第三步执行时据库崩溃：因为这个时候数据还没有发生任何变化，所以没有任何影响，不需要做任何操作。
- 在第四步修改内存中的记录时数据库崩溃：因为此时事务没有commit，所以这里要进行数据回滚，所以这里会通过undo log进行数据回滚
- 第五步写入binlog时数据库崩溃：这里和第四步一样的逻辑，此时事务没有commit，所以这里要进行数据回滚，会通过undo log进行数据回滚
- 执行第六步事务提交时数据库崩溃，要根据两种情况来做决定（binlog是否成功写入磁盘）：
  - 如果binlog存在事务记录：那么就**"认为"**事务已经提交了，这里可以根据redo log对数据进行重做（更新数据，与binlog一致）。其实你应该有疑问，其实这个阶段发生崩溃了，最终的事务是没提交成功的,这里应该对数据进行回滚。 这里主要的一个考虑是因为binlog已经成功写入了，而binlog写入后，那么依赖于binlog的其它扩展业务（比如：从库已经同步了日志进行数据的变更）数据就已经产生了，如果这里进行数据回滚，那么势必就会造成主从数据的不一致。
  - binlog不存在事务记录，那么这种情况事务还未提交成功，所以会对数据进行回滚。

#### change buffer

当需要更新一个数据页时，如果数据页在内存中就直接更新，如果不在内存中，在不影响数据一致性前提下，Innodb会将这些更新缓存到change buffer中，这样就不需要从磁盘中读这个数据页了。change buffer在内存中有拷贝也会被写入磁盘上。

使用场景：唯一索引不能使用change buffer，只有普通索引可以使用；写多读少的业务适用（如日志系统）

#### count

count()用于返回结果集的个数，如果count函数中参数为字段，会返回数据不为NULL的函数，因此尽量用count(*)、count(1)、count(主键ID)，count(1)要比count(主键ID)快，省去了每行ID对应值的解析；

效率排序：count(*)≈count(1)>count(主键ID)>count(字段)

#### 语法规则

group by要比order by先执行；order by不会对group by 内部进行排序；要查出group by中最大的或最小的某一字段使用 max或min函数；

#### group by与distinct

**DISTINCT：**这种方式会将全部内容存储在一个hash结构里，最后通过计算hash结构中key的个数即可得到结果，典型的以空间换取时间的方式；DISTINCT操作，它会读取所有记录；

**GROUP BY：**这种方式是先将字段排序（一般使用sort），然后进行计数，典型的以时间换取空间；GROUP BY需要读取的记录数量与分组的组数量一样多，也就是说比实际存在的记录数目要少很多；

我们可以得出，数据越是离散，DISTINCT需要消耗的空间越大，效率也就越低，此时GROUP BY的空间优势就显现了；相反，数据越是集中，DISTINCT空间占用变小，时间优势就显现出来了;

#### MySQL存储过程

将常用或复杂的工作预先用 SQL 语句写好并用一个指定名称存储起来，这个过程经编译和优化后存储在数据库服务器中，因此称为存储过程。

##### 优点

- **封装性**：存储过程被创建后，可以在程序中被多次调用，而不必重新编写该存储过程的 SQL 语句，并且数据库专业人员可以随时对存储过程进行修改，而不会影响到调用它的应用程序源代码
- **可增强 SQL 语句的功能和灵活性**：存储过程可以用流程控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算
- **可减少网络流量**：由于存储过程是在服务器端运行的，且执行速度快，因此当客户计算机上调用该存储过程时，网络中传送的只是该调用语句，从而可降低网络负载
- **高性能**：存储过程执行一次后，产生的二进制代码就驻留在缓冲区，在以后的调用中，只需要从缓冲区中执行二进制代码即可，从而提高了系统的效率和性能。
- **提高数据库的安全性和数据的完整性**：使用存储过程可以完成所有数据库操作，并且可以通过编程的方式控制数据库信息访问的权限

```sql
create procedure testa()
begin
    select * from users;
    select * from orders;
end

--调用
call testa();
```





### 事务

#### 特性

* **原子性**：事务中包含的程序作为数据库的逻辑工作单位，它对数据库中的数据进行操作时，要么全部执行，要么都不执行
* **一致性**：一个事务执行前和执行后，数据库都必须要处于一致性的状态。（你给小A的卡里转了500块，不管怎么样你卡里的钱和小A卡里的钱的总和是不变的。）
* **隔离性**：指在并发的事务是相互隔离的。即一个事务的内部操作及正在操作的数据必须被封锁起来，不会被其他的事务来企图修改。
* **持久性**：持久性是指当数据库系统出现故障了，要确保已经提交的事务的更新是不会丢失的。即数据库中的数据的修改是永久性的。就算系统出现了故障，我们也可以使用数据库的备份和恢复来保证数据的修改。

#### 事务的隔离级别

* 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他事务中未提交事务修改的数据。（会出现脏读、不可重复读、幻读）

* 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 。（会出现不可重复读、幻读）

  读提交隔离级别下还有一个优化，即：语句执行过程中加上的行锁（查询非索引字段时，逐行加锁），在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放了，不需要等到事务提交

* 可重复读(Repeated Read)：可重复读。同一事务中所有的 查询均读取第 一次读取时已确定的快照，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读。（但是还存在幻象读，GAP可以解决幻读）

  目前仅表数据支持可重复读，表结构不支持可重复读（仅支持当前读），MySQL8.0已经将表结构放在innodb字典里了，也许以后支持表结构的可重复读。

* 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞，在事务中的任何时候所看到的数据都是事务启动时刻的状态，不论在这期间有没有其他事务已经修改了某些数据并提交。

  对于高并发应用来说，为了尽可能保证数据的一致性，自然是事务隔离级别越高越好。但是，隔离级别越低，事务请求的锁越少或保持锁的时间就越短，性能越好。虽然 Innodb 存储引擎默认的事务隔离级别是 REPEATABLE READ，但实际上在我们大部分的应用场景 下，都只需要 READ COMMITED 的事务隔离级别就可以满足需求了。

  **脏读** :一个事务读取到另一事务未提交的更新数据
  **不可重复读 **: 在同一事务中,多次读取同一数据返回的结果有所不同, 换句话说, 后续读取可以读到另一事务已提交的更新数据. 相反, “可重复读”在同一事务中多次读取数据时, 能够保证所读数据一样, 也就是后续读取不能读到另一事务已提交的更新数据。
  **幻读 **:一个事务读到另一个事务已提交的insert数据

#### 事务的机制

事务的机制是通过视图来实现的并发版本控制(MVCC), 不同的事务隔离级别创建读视图的时间点不同。

innodb的读提交和可重复读是用一致性视图实现，在“可重复读”隔离级别下，这个视图是在事务启动[<B>第一次select</B>]时创建的，整个事务存在期间都用这个视图；在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的；“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

##### MySQL中，有两个视图的概念：

- 一个是view，它是用一个查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view……，他的查询方法与表一样。
- 另一个是InnoDB在实现MVCC时用到的<B>一致性读视图</B>，即consistent read view，用于支持RC（读提交）和RR（可重复读）隔离级别的实现。

#### 启动方式

1. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事物就启动了，而且并**不会自动提交**。这个事物持续存在直到你主动执行commit或rollback语句，或者断开连接；导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

2. set autocommit=1，显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是rollback；事务开始时需要主动执行一次“begin”，多一次交互。

   START TRANSACTION 后，只有当commit数据才会生效，ROLLBACK后就会回滚；如果没有START TRANSACTION ，执行每个SQL自动提交，调用ROLLBACK是没有用的。

   begin/start transaction命令并不是一个事务的起点，在执行到它们之后的第一个操作innodb表的语句（第一个快照读），事务才真正启动。如果你想马上启动一个事务，可以使用start transaction with consistent snapshot这个命令。

3. 在set autocommit=1情况下，用begin启动事务，执行commit work and chain，则是提交事务并启动下一个事务，这样省去了再次执行begin语句的开销。

#### MVCC原理

**多版本控制**: 指的是一种提高并发的技术。最早的数据库系统，只有读读之间可以并发，读写、写读、写写都要阻塞。引入多版本之后，**只有写写之间相互阻塞**，其他三种操作都可以并行，这样大幅度提高了InnoDB的并发度。

InnoDB 实现MVCC，是通过ReadView+ Undo Log 实现的，Undo Log 保存了历史快照，ReadView可见性规则帮助判断当前版本的数据是否可见。

在内部实现中，InnoDB通过undo log保存每条数据的多个版本，并且能够找回数据历史版本提供给用户读，每个事务读到的数据版本可能是不一样的。在同一个事务中，用户只能看到该事务创建快照之前已经提交的修改和该事务本身做的修改。

MVCC只在`已提交读`（Read Committed）和`可重复读`（Repeatable Read）两个隔离级别下工作，其他两个隔离级别和MVCC是不兼容的。因为未提交读，总数读取最新的数据行，而不是读取符合当前事务版本的数据行。而串行化（Serializable）则会对读的所有数据多加锁。

##### ReadView

ReadView是事务在进行快照读的时候生成的记录快照；

ReadView 的几个重要属性：

- **trx_ids**: 当前事务开启时，系统中那些活跃(未提交)的读写事务ID, 它数据结构为一个List。(`重点注意`:这里的trx_ids中的活跃事务，**不包括当前事务自己**和已提交的事务，这点非常重要)；可以理解为: **ReadView 保存了不应该让这个事务看到的其他事务 ID 列表**。
- **low_limit_id**: 目前出现过的最大的事务ID+1，即下一个将被分配的事务ID。
- **up_limit_id**: 活跃事务列表trx_ids中最小的事务ID，如果trx_ids为空，则up_limit_id 为 low_limit_id。
- **creator_trx_id**: 表示生成该 ReadView 的事务的 事务id

ReadView可见性规则:

- 如果被访问版本的 `事务ID = creator_trx_id`，那么表示当前事务访问的是自己修改过的记录，那么该版本对当前事务可见；

- 如果被访问版本的 `事务ID < up_limit_id`，那么表示生成该版本的事务在当前事务生成 ReadView 前已经提交，所以该版本可以被当前事务访问。

- 如果被访问版本的 `事务ID > low_limit_id` 值，那么表示生成该版本的事务在当前事务生成 ReadView 后才开启，所以该版本不可以被当前事务访问。

- 如果被访问版本的 `事务ID在 up_limit_id和m_low_limit_id` 之间，那就需要判断一下版本的事务ID是不是在 trx_ids 列表中，如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问；
  如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。

#### MVCC是怎么工作的

在可重复读隔离级别下，事务在启动[<B>第一次select</B>]的时候就拍了个快照，这个快照是基于整库的；注：如果想事务一启动就生成readview是当事务开启语句为start transaction with consistent snapshot。

innodb里面每个事务有一个唯一的事务ID，叫做transaction id，它是在事务开始的时候向innodb事务系统申请的，按申请顺序严格递增。

每行数据也都是有多个版本的，每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为row trx_id。

按照可重复的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后执行期间，其他事物的更新对他不可见。

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见之外，有三种情况：

- 版本未提交，不可见
- 版本已提交，但是是在视图创建后提交的，不可见
- 版本已提交，而且在视图创建前提交的，可见

可重复读的核心就是一致性读；而事务中update更新数据都是先读后写的，这个读，只能读当前的值，成为当前读；此外，select查询语句加锁（加上lock in share mode或for update），也是当前读。

读提交与可重复读的主要区别：

- 在可重复读隔离级别下，只需要在事务开始[<B>第一次select</B>]的时候创建一致性视图，之后事务里的其他查询共用这个一致性视图。
- 在读提交隔离级别下，每个语句执行前都会重新算出一个新的视图。

##### 如何查询一条记录

1.获取事务自己事务ID,即trx_id。(这个也不是select的时候获取的，而是这个事务开启的时候获取的 也就是begin的时候)

2.获取ReadView(这个才是select的时候才会生成的)

3.数据库表中如果查询到数据，那就到ReadView中的事务版本号进行比较。

4.如果不符合ReadView的可见性规则， 即就需要Undo log中历史快照,直到返回符合规则的数据;



### 存储引擎

#### MyISAM和InnoDb区别

* innodb支持事务
* innodb支持外键
* innodb支持行锁
* myisam数据和索引分开放，innodb数据和索引放一起；

- 都使用B+Tree作为索引结构，InnoDB的B+树主键索引的叶子节点就是数据文件，辅助索引的叶子节点是主键的值；而MyISAM的B+树主键索引和辅助索引的叶子节点都是数据文件的地址指针；

* myisam查询快，innodb写入快；

#### 为什么并发高时InnoDb写入快

* InnoDb支持行级锁，写入时只锁一行，myisam写入时会锁表
* 更新数据时，innodb只需要修改叶子节点，myisam需要修改索引结构（改动大）

#### 为什么MyISAM查询速度快（InnoDb维护的东西多）

* INNODB要缓存数据块，MYISAM只缓存索引块，这中间还有换进换出的减少
* innodb寻址要映射到块，再到行，MYISAM记录的直接是文件的OFFSET，定位比INNODB要快
* INNODB还需要维护MVCC（多版本并发控制）一致；

#### Innodb行级锁和事务性，容易产生死锁，优化建议：

* 类似业务模块中，尽可能按照相同的访问顺序来访问，防止产生死锁;
* 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率
* 对于非常容易产生 锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率

#### RocksDB引擎

- rocksdb压缩率非常高，大约只有innodb的1/3，同时也要比压缩后的innodb小。
- rocksdb读性能对比innodb还是差不少，但是跟压缩后的innodb相比，某些场景下，还是有一定优势。
- 写入性能非常优秀。
- 非常适合写多读少，并且对容量比较敏感的业务场景，如日志系统。



### 索引

#### 常见模型

##### 哈希表

哈希表是一种以键-值（key-value）存储数据的结构，把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。

多个key值经过哈希函数的换算，会出现同一个值的情况，处理这情况的方法是拉出一个链表；查找时，先定位到数组的位置，再去链表中顺序遍历。

key不是递增的，新增数据时速度快，只需往后追加，但缺点是，哈希索引做区间查询速度是很慢的，需要全部扫描。**哈希表适用于等值查询的场景**，比如memcached及一些NoSQL引擎。

##### 有序数组

数组根据唯一值按照递增顺序保存，无论查等值还是区间，用二分法查询，效率很高；仅看查询效率，有序数组是最好的数据结构了**有序数组在等值查询和范围查询场景中的性能都非常优秀**。

但是更新数据时，插入一条记录必须挪动后面的所有记录，成本太高。有序数据只适用于**静态存储引擎**，适合存放不会再修改的数据。

##### 树

h = log(m+1)N，m是数据块中数据项的个数，N是数据个数，h及查询次数，每一层的数据块中数据项是有顺序的（有序数组），全程使用二分查找；广泛应用于数据库引擎中。

#### innoDB的索引模型（B+树）
- 叶子结点：存储实际记录行，记录行比较紧密的存储，适合大数据量**磁盘存储**；
- 非叶子节点：不存储实际记录，只存储索引，用于查询加速，适合**内存存储**；

在InnoDB中，表都是根据主键顺序以索引的形式存放的，使用B+树索引模型，数据存放在B+树中。

根据叶子节点的内容，索引类型分为主键索引和非主键索引。

主键索引的叶子节点存放的是整行数据，在InnoDB中，主键索引又叫聚簇索引；如果表没有定义主键，则第一个非空unique列是主键索引，否则，InnoDB会创建一个隐藏的row-id作为主键索引。

非主键索引的叶子节点内容是主键的值，在InnoDB中，非主键索引又叫二级索引；基于非主键索引的查询需要多扫描一课索引树，因此尽量使用主键查询。

如果查询的字段已经在二级索引树上，就不需要回表，可以直接提供结果；这个索引树覆盖了查询需求，称为覆盖索引；可以通过建立联合索引（冗余索引）来支持覆盖索引。

联合索引（a,b,c）实质是按a、b、c顺序拼接成二进制字节数组，索引记录是按该字节数组逐字节比较排序的。

可以使用某个字段前N个字节index_field(N)设置为索引，使用前缀索引，定义好长度，就可以做到即可以节省空间，又不用额外增加太多的查询成本；但是使用前缀索引需要回表判断字段的值是否相等，不能使用覆盖索引。

##### B+树主键索引

主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小，从性能和空间方面看，自增主键是更好地选择。

适合业务字段直接做主键的场景：

1. 只有一个索引
2. 该索引是唯一索引

这就是典型的KV场景，由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。

##### 回表流程

1.根据二级索引定位到满足条件的主键，将主键id放入read_rnd_buffer中；

2.将read_rnd_buffer中的id进行递增排序；（注：按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能）

3.排序好的id数组，依次到主键索引树中查记录，并将结果返回；（注：在主键索引树上，每次只能根据一个主键 id 查到一行数据）

##### B+树页分裂

根据叶子节点结构，向上增加层数

https://www.cnblogs.com/qcfeng/p/6125465.html

##### 索引维护

索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，是的页面利用率更高；所以单重建二级索引可以省空间，但是重建主键索引会将整个表重建（同时影响其他索引，之后还得重建这些二级索引），可以直接使用：alter table T engine=InnoDB，重构所有索引。

truncate也可以清空索引空间，业务接受情况下可以代替delete。

##### 最左前缀原则

最左前缀可以是联合索引最左N个字段，也可以是最左m个字符。

当即有(a,b)联合查询，又有单独查询时，在空间考虑原则下，可以给小的字段创建一个单字段索引，再根据情况调整联合索引字段顺序。

##### 索引下推

使用联合索引时查询时，最左前缀可以定位记录，使用范围查询时（like、>=、<=等，测试时不带=号好像不能下推）时，范围查询字段后面的查询字段不符合最左前缀的要求：

在MySQL5.6之前，只能根据最左前缀查出匹配值后，一个个回表，从主键索引树找到数据行再比对后面查询的条件；

而MySQL5.6引入的索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

#### 什么情况下有索引，但用不上

1.条件中有or（部分条件带索引），如：select * from user where id=2 or name='zzq'，name无索引

2.组合索引，查询条件是后边字段，如：index(name,age)，select * from user where age=20

3.like查询以%开头（以%结尾可以用）

4.**对索引字段做函数操作，可能会破坏索引值的有序性，不会走索引（参数可以用函数）**

查询条件存在隐形转换，如：number varchar(10)，select * from user where number=123；注意由于MySQL规则将字符串转化为数据进行比较，对于优化器这个语句会成为select * from user where CAST(number AS signed int)=123；但是select * from user where id='2'会走索引，select * from user where id='2'（优化器转化为select * from user where id=CAST('2' AS signed int);）

5.如果数据特别少，MySQL优化器估计全表扫描更快，就不会走索引。



### 锁

#### 全局锁

给整个数据库实例加锁。

- MySQL提供一种方法，命令是 Flush tables with read lock (**FTWRL**)，整个库处于只读状态，典型使用场景是做全库逻辑备份。

- 官方自带的逻辑备份工具是mysqldump，当mysqldump使用参数-signle-transaction的时候，导数据之前就会启动一个事物，确保拿到一致性视图。**一致性读**是好，但前提是引擎要支持这个隔离级别，这种方法只适用于所有的表使用事务引擎的库。(InnoDB库使用这种更好)

#### 表级锁

##### 表锁
语法是 lock tables ... read/write。read会限制其他线程写操作，write会限制其他线程读操作。

##### 元数据锁(meta data lock，MDL)
MySQL5.5引入了MDL，是为了防止DDL（操作表结构语句）和DML（增删改查）的冲突，不需要显示使用；当对一个表进行增删改查操作时，加MDL读锁，读锁之间不互斥，多个线程可以对同一数据表做增删改查（不可以是同一行）；当对表结构变更操作时，加MDL写锁，当表结构变成完成后才会执行其他线程。

##### 意向共享锁（IS）
预示着事务有意向对表中某些行加共享锁（协议：事务要获得行的共享锁，必须先获得表的IS锁），意向锁之间不互斥，IS会与排它锁互斥
SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE，要设置IS锁

##### 意向排它锁（IX）
预示着事务有意向对表中某些行加排它锁（协议：事务要获得行的排它锁，必须先获得表的IX锁），意向锁之间不互斥，IX会与意向锁、排它锁互斥
SELECT * FROM table_name WHERE ... FOR UPDATE，要设置IX锁

##### 自增锁
自增锁是一个特殊的表级别锁，专门针对事务插入AUTO_INCREMENT类型的列（自增主键），最简单的情况，如果一个事物正在插入记录，所有其他事务的插入必须等待，以便第一个事务多次插入是连续的主键值。

#### 行锁

MySQL行锁是在各引擎层自己实现的，innoDB支持行锁，innodb行级锁是通过索引实现的；如果更新的列没建索引会锁整个表（根据主键索引逐行扫描 逐行加锁）。

session：select * from t where c=5 for update; 在 Read Committed 隔离级别下，会锁上聚簇索引中的所有记录（先锁所有记录，然后回server层判断，释放c!=5的行锁）；在 Repeatable Read 隔离级别下，会锁上聚簇索引中的所有记录(锁住所有行并不会释放)，并且会锁上聚簇索引内的所有 GAP；

在innodb事务中，行锁是在需要的时候加上去的，但并不是不需要了就立刻释放，而是要等到事务结束时才能释放，这个就是两阶段锁协议。

如果事务中需要锁多个行，要把最有可能造成索冲突、最影响并发度的锁尽量往后放。

##### 共享锁（s）
又称读锁。允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。
SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE。
##### 排他锁（Ｘ）
又称写锁。允许获取排他锁的事务更新数据，阻止其他事务取得相同的数据集共享读锁和排他写锁。SELECT * FROM table_name WHERE ... FOR UPDATE。

- update,delete,insert都会自动给涉及到的数据加上排他锁，select语句默认不会加任何锁类型
- 加过排他锁的数据行在其他事务种是不能修改数据的，也不能通过for update和lock in share mode锁的方式查询数据，但可以直接通过select …from…查询数据，因为普通查询没有任何锁机制
- select通过for update和lock in share mode读数据是当前读，读取当前最新数据；普通select是快照读，读取历史的快照，可提高并发。
- 如果当前事务也需要对该记录进行更新操作，则很有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用SELECT… FOR UPDATE方式获得排他锁
- InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！
- 在实际应用开发中，尤其是并发插入比较多的应用，我们要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件
- 选择合理的事务大小，小事务发生锁冲突的几率也更小

#### GAP间隙锁

当我们用范围条件而不是相等条件（唯一索引下）检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。（例：select * from  user where user_id > 100 for update;他会给符合条件不存在的记录加锁，insert into user values(102,'name')会被锁住）。

跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作，间隙锁之间不存在冲突关系。

间隙锁在InnoDB的唯一作用就是防止其它事务的插入操作，以此来达到**防止幻读**的发生，所以间隙锁不分什么共享锁与排它锁。

间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。

间隙锁是在**可重复读隔离**级别下才会生效的，要禁止间隙锁的话，可以把隔离级别降为读已提交，或者开启参数innodb_locks_unsafe_for_binlog（在my.cnf里面的[mysqld]添加innodb_locks_unsafe_for_binlog = 1）。

##### 插入意向锁
多个事务，在同一个索引，同一个范围区间插入记录时，如果插入的位置不冲突，不会阻塞彼此，可以提供插入并发。

#### 临键锁(Next-Key Locks)
临键锁，是行锁与间隙锁的组合，它的封锁范围，既包含索引记录，又包含索引区间；临键锁会封锁索引记录本身，以及索引记录之前的区间。

每个 next-key lock 是**前开后闭区间**，如果数据表中已存在ID（0,5,10,15,10,25），select * from t for update会把整个表锁起来，就形成了7个next-key lock，(-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。

#### 加锁规则

- 原则1：加锁的基本单位是 next-key lock，next-key lock 是前开后闭区间.
- 原则2：查找过程中访问到的对象才会加锁.
- 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁
- 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁
- 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止



#### 死锁

并发系统中，不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程进入无线等待的状态，称为死锁。

解决策略：

- 直接进入等待，直到超时。超时时间通过参数innodb_lock_wait_timeout来设置，默认值50s。
- 主动死锁检测，发现死锁后，主动回滚思索链条中的某一个事物，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑（默认是开启）。
- 控制并发



### 优化器

优化器选择索引的目的，是找一个最优的执行方案，用最小的代价执行语句。判断标准有：扫描行数、是否使用临时表、是否排序等。

扫描行数根据索引上不同值的个数（基数）决定。

当扫描行数信息不对时，可以使用analyze table t命令，来重新统计索引信息。

用explain查看优化器用错索引时，可以使用force index进行强行指定索引，也可以通过修改sql语句来优化，也可以增加或删除索引绕过这个问题。



### 优化

#### 数据库查询优化

* 永远用小结果集驱动大的结果集
* 只取出自己需要的 Columns
* 仅仅使用最有效的过滤条件
* 尽可能避免复杂的 Join 和子查询（减少锁表）,分解为几个小查询
* 使用count(*)获取行数效率高，count(列名)只记录非NULL的列
* like 'word%'可以用到索引，'%word%'用不到索引
* 分区partition by，分表分库

#### 建立索引原则

* 最左前缀匹配原则
* 尽量选择区分度高的列作为索引
* 索引列不能参与计算
* 尽量的扩展索引，不要新建索引
* 索引字段长度尽量的小
* 较频繁的作为查询条件的字段应该创建索引,更新非常频繁的字段不适合创建索引

#### 字符串索引优化

- 直接创建完整索引，这样可能比较占用空间
- 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引
- 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题
- 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。

### 分区

MySQL5.1及以上支持分区；

对于Server层，分区表还是一个表；
对于存储引擎层，分区表会是多张表；

#### 分区方式

- range 分区，这种模式允许将数据划分不同范围。例如可以将一个表通过年份划分成若干个分区

  ```
  create table t_range( 
  　　   id int(11), 
  　　   date datetime 
  　　)partition by range(year(date))( 
  　　partition p2007 values less than (2008), 
  　　partition p2008 values less than (2009), 
  　　partition p2010 values less than maxvalue  #MAXVALUE 表示最大的可能的整数值
  )；
  ```

- list分区，这种模式允许系统通过预定义的列表的值来对数据进行分割

  ```
  create table t_list( 
  　　a int(11), 
  　　b int(11) 
  　　)(partition by list (b) 
  　　partition p0 values in (1,3,5,7,9), 
  　　partition p1 values in (2,4,6,8,0) 
  );
  ```

- hash分区，这种模式允许通过对表的一个或多个列的Hash Key进行计算（常规hash是基于分区个数的取模%运算），最后通过这个Hash码不同数值对应的数据区域进行分区。

  ```
  CREATE TABLE employees (
      id INT NOT NULL,
      store_id INT
  )
  PARTITION BY HASH(store_id)
  PARTITIONS 4;
  ```

- key分区，上面Hash模式的一种延伸，这里的Hash Key是MySQL系统产生的，不允许使用用户自定义的表达式进行分区。

  ```
  CREATE TABLE tk (
      col1 INT NOT NULL
  )
  PARTITION BY LINEAR KEY (col1)
  PARTITIONS 3;
  ```

#### 分区表的劣势

- MySQL 在第一次打开分区表的时候，需要访问所有的分区——打开的表较多，性能糟糕也可能报打开的表超过设置的问题；因此，不能建立太多的分区。
- 在 server 层，认为这是同一张表，因此所有分区共用同一个 MDL 锁——锁粒度大，DDL耗时是非常严重的，影响并发度，站在Server看也是合理的，不过站在存储引擎的角度看就不合理了。
- 在引擎层，认为这是不同的表，因此 MDL 锁之后的执行过程，会根据分区表规则，只访问必要的分区——被访问到的分区。

### 架构

#### MySQL主从复制和读写分离

MySQL读写分离基本原理是让master数据库处理写操作，slave数据库处理读操作。master将写操作的变更同步到各个slave节点。MySQLProxy实际上是在客户端请求与MySQLServer之间建立了一个连接池。所有客户端请求都是发向MySQLProxy，然后经由MySQLProxy进行相应的分析，判断出是读操作还是写操作，分发至对应的MySQLServer上。对于多节点Slave集群，也可以起做到负载均衡的效果。

MySQL读写分离能提高系统性能的原因在于：

* 物理服务器增加，机器处理能力提升。拿硬件换性能。
* 主从只负责各自的读和写，极大程度缓解X锁和S锁争用。
* slave可以配置myisam引擎，提升查询性能以及节约系统开销。
* master直接写是并发的，slave通过主库发送来的binlog恢复数据是异步。
* slave可以单独设置一些参数来提升其读的性能。
* 增加冗余，提高可用性。

#### 主从同步原理

主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写 binlog；备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：

1.在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。

2.在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。

3.其中 io_thread 负责与主库建立连接。主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。

4.备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。

5.sql_thread 读取中转日志，解析出日志里的命令，并执行。后来由于多线程复制方案的引入，sql_thread 演化成为了多个线程。

#### 主从延迟

数据同步有三个时间点：

1.主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1;

2.之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2;

3.备库 B 执行完成这个事务，我们把这个时刻记为 T3。

在网络正常的时候，日志从主库传给备库所需的时间是很短的，即 T2-T1 的值是非常小的。也就是说，网络正常情况下，主备延迟的主要来源是备库接收完 binlog 和执行完这个事务之间的时间差。

所以，主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产 binlog 的速度要慢。

##### 主从延迟的来源：

- 备库所在机器的性能要比主库所在的机器性能差；实际上，更新过程中也会触发大量的读操作，备库有读压力，导致同步延迟；

  解决方案：主备使用相同规格的机器，对称部署；

- 备库提供大量读能力，执行数据分析，导致耗费了大量CPU，影响了同步速度，造成主备延迟。

  解决方案：1.一主多从，多台备库分担读压力；2.通过 binlog 输出到外部系统，比如 ES 这类系统，让外部系统提供统计类查询的能力。

- 大事务，删除大量数据，或者大表的DDL；

  解决方案：分批删除 ；

- 在官方的 5.6 版本之前，MySQL 只支持单线程复制，备库执行日志的速度持续低于主库生成日志的速度，由此在主库并发高、TPS 高时就会出现严重的主备延迟问题；

  解决方案：将sql_thread，拆成多个线程，coordinator 就是原来的 sql_thread, 不过现在它不再直接更新数据了，只负责读取中转日志和分发事务。真正更新日志的，变成了 worker 线程， 线程数设置成CPU核数的一半以内（毕竟备库还有可能要提供读查询，不能把 CPU 都吃光）；

  coordinator 在分发的时候，需要满足以下这两个基本要求：

  1.更新同一行的两个事务，必须被分发到同一个 worker 中；（如果不同worker执行同一行上的两个事务，在主库和备库上的执行顺序相反，会导致主备不一致的问题。）

  2.同一个事务不能被拆开，必须放到同一个 worker 中（一个事务更新了表 t1 和表 t2 中的各一行，如果这两条更新语句被分到不同 worker 的话，虽然最终的结果是主备一致的，但如果表 t1 执行完成的瞬间，备库上有一个查询，就会看到这个事务“更新了一半的结果”，破坏了事务逻辑的隔离性）。



#### MySQL安全设置

* 定期做数据备份
* 不给root权限，合理安排权限
* 关闭远程访问
* 设置MySQL数据文件的权限



### 日常排查

mysql查看当前连接数：进入mysql命令行执行 show processlist; 可以显示前100条连接信息 show full processlist; 可以显示全部。随便说下，如果用普通账号登录，就只显示这用户的。



### 日常使用

#### 删除

- drop table命令，可以将.ibd（表数据文件）删除；
- delete命令只是把记录的位置或者数据页标记为“可复用”，但磁盘文件大小是不会变的，这时未使用的空间就是空洞；插入操作造成的页分裂也会产生空洞。
- alter table A engine=InnoDB命令可以重建表，对于很大的表来说，很消耗IO和CPU，要在业务低峰时使用；也可以建一个相同表结构的B表，将A表数据迁移到B后，业务逻辑再切到B表。

#### Online DDL的流程：

1.建立临时文件，扫描表A主键所有数据页

2.用表A的记录生成B+树，存在临时文件中

3.生成临时文件过程中，将A表的操作记录存于一个日志文件中

4.临时文件生成后，将日志文件应用到临时文件，得到逻辑结构上与表A相同的数据文件

5.用临时文件代替表A的数据文件

#### 重建表的不同方式

- 从MySQL5.6开始，alter table t engine=InnoDB（也就是recreate）默认就是以上的流程了
- analize table t 其实不是重建表，只是对表的索引重新统计，没有修改数据，这个过程加了MDL读锁
- optimize table t 等于 recreate+analize
- truncate可以理解为drop+create
