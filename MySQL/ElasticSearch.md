# 第五节：ElasticSearch

### 基础

ES不是数据库，它适合于海量数据、更新频率很低的数据(ES没有事务也不适合处理并行更改数据)，创建文档后，es不支持更新域（字段类型）。
ES是全文搜索引擎，能够对中文进行全文搜索，处理同义词和根据相关性给文档打分；能根据同一份数据生成分析和聚合的结果；在没有大量工作进程（线程）的情况下能做到对数据的实时处理。

#### 字段类型

ElasticSearch 5.0以后，字符串类型有重大变更，移除了string类型，string字段被拆分成两种新的数据类型， text和keyword：

**text**：会分词，然后进行索引，用于全文搜索；支持模糊、精确查询；不支持聚合（使用keyword进行聚合）。

**keyword**：不进行分词，直接索引，keyword用于关键词搜索；支持模糊、精确查询；支持聚合。

#### 检索

term会精准查询，match会进行分词查询；

index设置成false，不支持搜索，支持terms聚合；

enable设置成false，将无法进行搜索和聚合分析；

#### Mapping

Dynamic = true：一旦有新增字段写入，Mapping也会被更新

Dynamic = false：Mapping不会被更新，新增字段的数据无法被索引，但是信息会出现在_source中

Dynamic = strict：文档写入新字段时，直接失败

#### 索引

ES 作为搜索引擎，使用的就是倒排索引；ES对文档每个字段都有自己的倒排索引，可以指定某些字段不做索引，这样可以节省存储空间，缺点是这个字段无法被搜索；

在介绍倒排索引前先介绍下正向索引；

##### 正向索引

在搜索引擎中每个文件都有对应的一个文件ID，文件内容被表示为一系列关键词的集合（实际上在搜索引擎库中，关键词也已经转换为关键词ID）**例如**： 文档1 经过分词，提取出了十个关键词，每个关键词都会记录他在文档中出现的次数，出现的位置，得到**正向索引**如下：

```
文档1 的ID-----》关键词1（出现的次数、位置）；关键词2（出现的次数、位置）；.....
文档2的ID。。。。
```

当搜索"ElasticSearch"时，如果只是用正向索引的话，按照上图，就需要扫描所有索引库中的所有文档，找出所有包含"ElasticSearch" 关键词的文档，然后根刺打分模型进行打分，排出名次后呈现给用户。

可以看出**正向索引**使用的是一种文档到关键字的方式 document -> to -> words。

##### 倒排索引

在创建索引之前，会对文档中的字符串进行分词。ES中字符串有两种类型，keyword和text。

- keyword类型的字符串不会被分词，搜索时全匹配查询
- text类型的字符串会被分词，搜索时是包含查询

倒排索引由两部分构成：

- **单词词典：**保存索引的最小单位，文档集合中所有单词的集合，记录指向倒排列表的指针；单词词典有两种数据结构实现：**B+树**和**Hash表**；

- **倒排列表：**记录出现过某个单词的文档列表，同时还记录单词所在文档中的出现次数和偏移位置；

  倒排列表元素数据结构：(DocID;TF;<POS>)

  - DocID：出现某单词的文档ID

  - TF(Term Frequency)：单词在该文档中出现的次数

  - POS：单词在文档中的位置

    | 单词ID | 单词 | 逆向文档频率 | 倒排列表(DocID;TF;<POS>)      |
    | ------ | ---- | ------------ | ----------------------------- |
    | 1      | 目标 | 3            | (1;1;<3>),(2;1;<5>),(3;1;<4>) |
    | 2      | 年度 | 3            | (1;1;<2>),(2;1;<4>),(3;1;<3>) |
    | 3      | AI   | 2            | (2;1;<1>),(3;1;<1>)           |

  比如单词“年度”，单词ID为2，在三个文档中出现过，所以逆向文档频率为3，同时倒排索引中的元素也有三个：`(1;1;<2>),(2;1;<4>),(3;1;<3>)`。拿第一个元素`(1;1;<2>)`进行说明，他表示“年度”再文档ID为1的文档中出现过1次，出现的位置是第二个单词；

倒排索引的搜索过程：

查询单词词典 -》 获取单词在倒排列表的指针 -》 通过倒排列表获取ID列表

##### 倒排索引不可变性

倒排索引采用Immutable Design，一旦生成，不可更改。

优点：

- 无需考虑并发写文件的问题，避免了锁机制带来的性能问题。

- 一旦读入内核的文件系统缓存，便留在那里。只要文件系统存有足够大的空间。大部分请求就会直接请求内存，不会命中磁盘，提升了很大的性能。

缺点：

不可变性也带来了另一个挑战，如果需要让一个新的文档可以被索引，需要重建整个索引。

**es为了应对倒排索引不可更新：**采用方式是**增加新的索引**来反映最近的变化，然后查询的时候一次查询所有的倒排索引，从最早的一直到最新的，然后在**合并结果**返回。

#### 集群
集群这个概念从业务层面来看，是聚合了某方面功能的节点的组合，一般由单个或多个节点组成。在ES中，具有相同的cluster.name的节点就为同一个集群， 一个集群就是由一个或多个拥有相同cluster.name的节点组成。
#### 节点
一个运行中的ES实例被成为一个节点，一个或多个节点（ES实例）则组成一个集群。
每个集群中会有主节点，ES会自动进行主节点选举，对用户透明。主节点的主要职责是管理集群范围內的数据变更，包括索引的增加／删除 ，节点的增加／删除，监控节点的状态等，它的工作负荷相对其他节点而言较轻，当集群流量增加时，它不会成为瓶颈。从节点负责具体的细节变更，比如文档级别的变更或搜索。各个节点能均匀地分摊集群的负载，实现负载均衡，以及数据冗余备份，当某个节点因为硬件故障丢失数据时，其他节点存放的冗余数据可以保证整个集群的数据完整性。

**客户端节点(协调节点)：**该节点只能处理路由请求，处理搜索，分发索引操作等；提供负载均衡功能；

**数据节点：**主要是存储索引数据的节点，**主要对文档进行增删改查操作，聚合操作**等。数据节点对cpu，内存，io要求较高， 在优化的时候需要监控数据节点的状态，当资源不够的时候，需要在集群中添加新的节点；

**主节点：**主要职责是和集群操作相关的内容，如**创建或删除索引，跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点**；稳定的主节点对集群的健康是非常重要的，默认情况下任何一个集群中的节点都有可能被选为主节点，索引数据和搜索查询等操作会占用大量的cpu，内存，io资源，为了确保一个集群的稳定，分离主节点和数据节点是一个比较好的选择。  

#### 分片
分片是一个底层的工作单元每个分片都是一个Lucene实例，每个Lucene实例都是一个独立的搜索引擎每个分片只存放索引全部数据中的一部分；
分片是节点中存储数据的核心单元，每个索引的文档数据是均匀地存放到多个分片中的（默认为5个），每个分片只能存储20亿个文档；一个集群是由一个或多个节点（ES实例）组成，一个节点又是由一个或多个分片（Lucene实例）组成；

分片有主分片和副本分片的区别，索引的内容均匀地放到各个主分片中，每个文档都属于某一个主分片；副本分片是主分片的冗余拷贝，和对应的主分片有完全一样的数据；主分片的数量在索引建立时会被定死，副本分片的数量没有限制，副本分片越多，集群的规模会相应的扩大，海量的检索请求会分散到各个副本上，系统的吞吐量会有成倍的提升。主分片数在索引创建后，（由于文档路由已经定了）不能修改，只能reindex；

#### 副本的作用

- 故障转移/集群恢复：海量的检索请求会分散到各个副本上，系统的吞吐量会有成倍的提升

- 通过副本进行负载均衡：此外副本策略提供了高可用和数据安全的保障，当分片所在的机器宕机，Elasticsearch可以使用其副本进行恢复，从而避免数据丢失。 

#### 写入数据的底层原理

**1）**客户端发起数据写入请求，根据**routing规则选择发给哪个分片**；确认Index Request中是否设置了使用Filed的值作为路由参数，如果没有设置，则使用Mapping中的配置，如果mapping中也没有配置，则使用_id作为路由参数，然后通过_routing的Hash值（Hash值与分片量取模）选择出Shard，最后从集群的Meta中找出出该Shard的Primary节点。

**2）**写入请求到达Shard后，**先把数据写入到内存队列（buffer）中，此时数据是搜索不到的，同时会写入一条日志到translog日志文件中去**。
**3）** **执行refresh操作**：buffer满了或者每隔1秒(可配)，将mem buffer中的数据生成index segment文件并写入os cache，此时index segment可被打开以供search查询读取，这样文档就可以被搜索到了（注意，此时文档还没有写到磁盘上），然后清空mem buffer供后续使用；

**refresh实现的是文档从内存移到文件系统缓存的过程**；

通过es的restful api或者java api，手动执行一次refresh操作，就是手动将buffer中的数据刷入os cache中，让数据立马就可以被搜索到。
**4）**重复上两个步骤，新的segment不断添加到os cache，mem buffer不断被清空，而translog的数据不断增加，随着时间的推移，translog文件会越来越大；

translog本身也是磁盘文件，频繁的写入磁盘会带来巨大的IO开销，因此对translog的追加写入操作的同样操作的是os cache，也需要定时落盘（fsync）。可以将translog设置成每次写操作必须是直接fsync到磁盘，但是性能会差很多。
**5）**默认每隔30分钟或者当tanslog的大小达到512M时候，flush操作，将os cache中所有的数据全以segment file的形式，持久到磁盘上去。
​	5.1）将buffer中现有数据生成segment file并refresh到os cache中，让数据可以被搜索，然后清空buffer ；

​	5.2）os cache中缓存的所有的index segment文件被fsync强制刷到磁盘os disk，之后index segment就会被打开，供查询使用；

​	5.3）一个commit point被写入磁盘，这个commit point中标明所有的index segment；

​	5.4）translog被清空和删除，创建一个新的translog；

##### translog的功能

- 保证在filesystem cache中的数据不会因为elasticsearch重启或是发生意外故障的时候丢失。
- 当系统重启时会从translog中恢复之前记录的操作。
- 当对elasticsearch进行CRUD操作的时候，会先到translog之中进行查找，因为tranlog之中保存的是最新的数据。
- translog的清除时间时进行flush操作之后（将数据从filesystem cache刷入disk之中）。

#### 搜索的底层原理

查询过程大体上分为**查询**和**取回**这两个阶段，广播查询请求到所有相关分片，并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。

- 查询阶段
  - 当一个节点接收到一个搜索请求，这这个节点就会变成协调节点，第一步就是将广播请求到搜索的每一个节点的分片拷贝，查询请求可以被某一个主分片或某一个副分片处理，协调节点将在之后的请求中轮训所有的分片拷贝来负载均衡。
  - 每一个分片将会在本地构建一个优先级队列，如果客户端要求返回结果排序中从from 名开始的数量为size的结果集，每一个节点都会产生一个from+size大小的结果集，因此优先级队列的大小也就是from+size，分片仅仅是返回一个**轻量级**的结果给协调节点，包括结果级中的每一个文档的ID和进行排序所需要的信息。
  - 协调节点将会将所有的结果进行汇总，并进行全局排序，最总得到排序结果。
- 取值阶段
  - 协调节点会确定实际需要的返回的文档，根据唯一标识去各个节点进行拉取数据，分片获取的文档返回给协调节点，协调节点将结果返回给客户端。

#### 查询语句

**match：**基于全文的查询，先对输入的查询进行分词，然后对每个词逐个查询，最后将结果合并；

**match_phrase：**基于全文的查询，但是会精确匹配位置；

**term：**完全匹配，即不进行分词器分析，文档中必须包含整个搜索的词汇

注：如果字段设置了keyword，你用term查询，就会精确匹配。例如说keyword字段，索引时是“Iphone”，你的term查询必须是Iphone，输入“iphone”就无法匹配；而如果你的字段是“text”类型。你index时候，如果是“Iphone”，在term查询时，“iphone”可以匹配。但是，“Iphone”不会（背后的原因是，text类型的数据会分词，默认分词器会将输入一个个单词切开，并且转小写了，所以你 term查询时，必须用“iphone”）。



### 优化

#### 海量数据如何提高效率

**1.filesystem cache：**ES的搜索引擎是严重的依赖底层的filesystem cache，如果给filesystem cache更多的内存，尽量让内存可以容纳所有的index segment file 索引数据文件。

**2.数据预热：**对于那些你觉得比较热的数据，经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据，每隔一段时间，你就提前访问以下，让数据进入filesystem cache里面去，这样期待下次访问的时候，性能会更好一些。

**3.冷热分离：**关于ES的性能优化，数据拆分，将大量的搜索不到的字段，拆分到别的存储中去，这个类似于MySQL的分库分表的垂直拆分。

### 坑

#### 分页
ES默认的分页机制一个不足的地方是，比如有5010条数据，当你仅想取第5000到5010条数据的时候，ES也会将前5000条数据加载到内存当中，ES为了避免用户的过大分页请求造成ES服务所在机器内存溢出，默认对深度分页的条数进行了限制，默认的最大条数是10000条。
解决方案1：
修改配置，调整的新的窗口数：

```
curl -XPUT http://127.0.0.1:9200/my_index/_settings -d '{ "index" : { "max_result_window" : 500000}}'。
```
缺点：窗口值调大了后，虽然请求到分页的数据条数更多了，但它是用牺牲更多的服务器的内存、CPU资源来换取的。
解决方案2：
https://www.elastic.co/guide/en/elasticsearch/reference/5.6/search-request-search-after.html，利用search_after参数
scroll查询原理是在第一次查询的时候一次性生成一个快照，根据上一次的查询的id来进行下一次的查询，这个就类似于关系型数据库的游标，然后每次滑动都是根据产生的游标id进行下一次查询，这种性能比上面说的分页性能要高出很多，基本都是毫秒级的。
缺点：不支持跨页查询
解决方案3：
每次查询超过10000万条记录的时候，都会去更新一次index。
缺点：慢

### 命令

查看已装插件：bin/elasticsearch-plugin list

安装插件：bin/elasticsearch-plugin install analysis-icu

开启多节点（同一机器开启多节点，配置文件中不要指定端口，Elasticsearch 会取用9200~9299这个范围内的端口，如果9200被占用，就选择9201，依次类推）：

bin/elasticsearch -E node.name=node0 -E cluster.name=zzq -E path.data=node0_data -E http.port=9200 -d

bin/elasticsearch -E node.name=node1 -E cluster.name=zzq -E path.data=node1_data -E http.port=9201 -d

### cerebro（ES监控工具）

运行：bin/cerebro

指定端口运行：bin/cerebro -Dhttp.port=9100

浏览器打开：http://127.0.0.1:9100

### logstash（导入数据到ES，注：两者版本一致）

导入数据：sudo bin/logstash -f logstash.conf（注：需要使用 sudo ，否则命令执行完成后 没有权限操作相关文件）

### 版本更新

ES从5.X就引入了text和keyword，其中keyword用于不分词字段，搜索时只能完全匹配；到了6.X就彻底移除string了，“index”的值就只能是boolean变量了。

### 日常问题

#### 将分片分配给node节点

```
POST 10.66.96.204:9200/_cluster/reroute
{
	"commands":[{
		"allocate_stale_primary":{
			"index":"index_sms_long",//索引名称
			"shard": 22,//分片序号
			"node":"dvygzuZ5SYejNREJ7ohE3w",//node节点
			"accept_data_loss":true
		}
	}
		]
}
```

